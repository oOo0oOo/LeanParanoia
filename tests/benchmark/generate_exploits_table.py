#!/usr/bin/env python3
"""Generate comparison table for VERIFIER_COMPARISON.md from benchmark results."""

import json
from pathlib import Path


def format_cell(data: dict, tool: str, extra_time: str = "") -> str:
    """Format a table cell with status, time, and optional details."""
    status = {"yes": "ðŸ›‘", "no": "ðŸŸ¢"}.get(data["detected"], "ðŸŸ¡")
    msg = data.get("message", "")

    # Clean up message
    if msg and msg.startswith("PANIC "):
        for line in msg.split("\n"):
            if line.startswith("PANIC at "):
                msg = "lean4export: " + line.strip()
                break
    elif msg and "uncaught exception:" in msg:
        msg = msg.split("uncaught exception:")[-1].strip()
        if msg:
            msg = msg.split("\n")[0]

    # Filter out linter warnings when status is passed (no exploit detected)
    if status == "ðŸŸ¢" and msg and ("linter" in msg.lower() or "note:" in msg.lower()):
        msg = ""

    time = f"{data['time_ms']}ms" if data.get("time_ms") else "N/A"
    time_display = f"{time} ({extra_time})" if extra_time else time

    if msg and msg not in ["skipped", "not tested", "n/a"]:
        return f"<details><summary>{status} {time_display}</summary>{msg}</details>"
    return f"{status} {time_display}"


def generate_table(results: dict) -> str:
    """Generate markdown table from benchmark results."""
    table_parts = [
        "## Tool Comparison Results\n",
        "| Exploit | LeanParanoia | lean4checker | SafeVerify | Comparator |",
        "|---------|--------------|--------------|------------|------------|",
    ]

    for category in sorted(results):
        for exploit in results[category]:
            name = exploit["exploit_file"]
            link = f"{name} [â†—](tests/lean_exploit_files/{name}.lean)"

            lp = exploit["leanparanoia"]
            lp_extra = f"{lp['time_failfast_ms']}ms" if lp.get("time_failfast_ms") else "N/A"
            lp_cell = format_cell(lp, "leanparanoia", lp_extra)

            l4c_cell = format_cell(exploit["lean4checker"], "lean4checker")
            sv_cell = format_cell(exploit["safeverify"], "safeverify")

            cmp = exploit.get("comparator", {"detected": "n/a", "message": "not tested", "time_ms": None})
            cmp_cell = format_cell(cmp, "comparator")

            table_parts.append(f"| {link} | {lp_cell} | {l4c_cell} | {sv_cell} | {cmp_cell} |")

    legend = """
---

### Legend

- ðŸ›‘ **Detected**: Tool identified an exploit
- ðŸŸ¢ **Passed**: Tool did not detect any exploit
- ðŸŸ¡ **N/A**: Test setup issue or methodology limitation
- <details><summary>Click to show details</summary>Verification results or tool output</details>


### Timing

- **LeanParanoia**: Verification of pre-compiled `.olean` files (fail-fast time in brackets)
- **Lean4checker**: Kernel re-verification of pre-compiled `.olean` files
- **SafeVerify**: Comparison of pre-compiled `.olean` files
- **Comparator**: Equivalence checking of pre-compiled `.olean` files

Note: WIP, might contain inaccuracies.
"""
    return "\n".join(table_parts) + legend


def main():
    """Generate and update comparison table."""
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent.parent
    results_path = script_dir / "comparison_results.json"
    exploits_path = root_dir / "VERIFIER_COMPARISON.md"

    if not results_path.exists():
        print(f"Error: {results_path} not found")
        print("Run the benchmark test first:")
        print("  uv run pytest tests/benchmark/test_tool_comparison.py -m benchmark_comparison")
        return 1

    results = json.loads(results_path.read_text())
    table = generate_table(results)

    content = f"""# Exploit Test Files

**Note**: Each tool is designed for different purposes and performing according to its specifications.

{table}
"""
    exploits_path.write_text(content)

    print(f"âœ“ Updated {exploits_path} with comparison table")
    print(f"  Found {sum(len(v) for v in results.values())} exploit comparisons")
    return 0


if __name__ == "__main__":
    exit(main())
