#!/usr/bin/env python3
"""Generate comparison table for VERIFIER_COMPARISON.md from benchmark results."""

import json
from pathlib import Path


def load_results(json_path: Path) -> dict:
    """Load comparison results from JSON file."""
    with open(json_path) as f:
        return json.load(f)


def shorten_message(msg: str, tool: str) -> str:
    """Shorten long messages for display in table."""
    if not msg:
        return ""

    if tool == "safeverify":
        # Extract the key error from SafeVerify output before the name lookup error
        lines = msg.split("\n")

        # Look for specific error patterns in order of priority
        for line in lines:
            if "is not in the allowed set of standard axioms" in line:
                # Extract just the error part after the colon
                parts = line.split(":")
                if len(parts) > 1:
                    return parts[-1].strip()
                return line.strip()

        # Look for sorryAx detection
        for i, line in enumerate(lines):
            if "#[sorryAx]" in line:
                # Check if this is in the submission (not target)
                # Look backwards to see if it's the theorem name we care about
                for j in range(max(0, i-5), i):
                    if "LeanTestProject." in lines[j] and "theorem" in lines[j-1]:
                        return "sorryAx detected in proof"

        # Look for kernel/interpreter errors
        for line in lines:
            if "uncaught exception:" in line and "not found in submission" not in line:
                error = line.split("uncaught exception:")[-1].strip()
                if error:
                    # Remove path prefixes
                    error = error.replace("/home/ooo/Code/LeanParanoia/.pytest_cache/LeanTestProject/", "")
                    # Truncate long errors
                    if len(error) > 80:
                        error = error[:77] + "..."
                    return error

        return ""

    elif tool == "lean4checker":
        # Extract the key error
        if "uncaught exception:" in msg:
            error = msg.split("uncaught exception:")[-1].strip()
            if len(error) > 80:
                error = error[:77] + "..."
            return error
        elif "found a problem" in msg:
            return msg.split("\n")[0]  # First line only
        return msg[:80] + "..." if len(msg) > 80 else msg

    else:  # leanparanoia
        return msg
def generate_table(results: dict) -> str:
    """Generate markdown table from benchmark results."""
    lines = [
        "## Tool Comparison Results",
        "",
        "| Exploit | LeanParanoia | lean4checker | SafeVerify |",
        "|---------|--------------|--------------|------------|",
    ]

    for category in sorted(results.keys()):
        for exploit in results[category]:
            name = exploit["exploit_file"]

            # LeanParanoia - ðŸ›‘ means detected (good), ðŸŸ¢ means passed (bad for exploits)
            lp = exploit["leanparanoia"]
            lp_status = "ðŸ›‘" if lp["detected"] == "yes" else "ðŸŸ¢" if lp["detected"] == "no" else "ðŸŸ¡"
            lp_msg = shorten_message(lp["message"], "leanparanoia")
            lp_time_full = f"{lp['time_ms']}ms" if lp.get("time_ms") else "N/A"
            lp_time_fast = f"{lp['time_failfast_ms']}ms" if lp.get("time_failfast_ms") else "N/A"
            lp_cell = f"{lp_status} {lp_time_full} ({lp_time_fast})"
            if lp_msg:
                lp_cell += f"<br>{lp_msg}"

            # lean4checker - ðŸ›‘ means detected (good), ðŸŸ¢ means passed (bad for exploits)
            l4c = exploit["lean4checker"]
            l4c_status = "ðŸ›‘" if l4c["detected"] == "yes" else "ðŸŸ¢" if l4c["detected"] == "no" else "ðŸŸ¡"
            l4c_msg = shorten_message(l4c["message"], "lean4checker")
            l4c_time = f"{l4c['time_ms']}ms" if l4c.get("time_ms") else "N/A"
            l4c_cell = f"{l4c_status} {l4c_time}"
            if l4c_msg:
                l4c_cell += f"<br>{l4c_msg}"

            # SafeVerify - ðŸ›‘ means detected (good), ðŸŸ¢ means passed (bad for exploits)
            sv = exploit["safeverify"]
            sv_status = "ðŸ›‘" if sv["detected"] == "yes" else "ðŸŸ¢" if sv["detected"] == "no" else "ðŸŸ¡"
            sv_msg = shorten_message(sv["message"], "safeverify")
            sv_time = f"{sv['time_ms']}ms" if sv.get("time_ms") else "N/A"
            sv_cell = f"{sv_status} {sv_time}"
            if sv_msg:
                sv_cell += f"<br>{sv_msg}"
            elif sv_status == "ðŸ›‘":
                sv_cell += f"<br>exploit detected"
            elif sv_status == "ðŸŸ¡":
                sv_cell += f"<br>error during verification"

            lines.append(f"| {name} | {lp_cell} | {l4c_cell} | {sv_cell} |")

    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("### Legend")
    lines.append("")
    lines.append("- ðŸ›‘ **Detected**: Tool identified an exploit")
    lines.append("- ðŸŸ¢ **Passed**: Tool did not detect any exploit")
    lines.append("- ðŸŸ¡ **N/A**: Test setup issue or methodology limitation")
    lines.append("")
    lines.append("**Time Format**: For LeanParanoia: `full_time (fail-fast_time)`")
    lines.append("")
    lines.append("Note: WIP, might contain inaccuracies.")

    return "\n".join(lines)
def update_comparison_md(exploits_path: Path, table: str):
    """Update VERIFIER_COMPARISON.md with the comparison table."""
    # Just keep the title and replace with the table
    lines = [
        "# Exploit Test Files",
        "",
        "**Note**: Each tool is designed for different purposes and performing according to its specifications.",
        "",
        table
    ]
    exploits_path.write_text("\n".join(lines))


def main():
    """Generate and update comparison table."""
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent.parent

    results_path = script_dir / "comparison_results.json"
    exploits_path = root_dir / "VERIFIER_COMPARISON.md"

    if not results_path.exists():
        print(f"Error: {results_path} not found")
        print("Run the benchmark test first:")
        print("  uv run pytest tests/benchmark/test_tool_comparison.py -m benchmark_comparison")
        return 1

    results = load_results(results_path)
    table = generate_table(results)
    update_comparison_md(exploits_path, table)

    print(f"âœ“ Updated {exploits_path} with comparison table")
    print(f"  Found {sum(len(v) for v in results.values())} exploit comparisons")

    return 0


if __name__ == "__main__":
    exit(main())
