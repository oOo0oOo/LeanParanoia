"""Regression tests for native_decide and related axioms."""

import pytest


@pytest.mark.exploit
def test_native_decide_axiom_rejected(verifier):
    """Test native_decide rejected with empty axiom whitelist."""
    config = {"allowedAxioms": []}
    result = verifier.verify_theorem(
        "LeanTestProject.NativeDecide.Axiom", "exploit_theorem", config=config
    )

    assert not result.success
    assert "AxiomWhitelist" in result.failed_tests


@pytest.mark.exploit
def test_native_decide_leak_rejected(verifier, default_config):
    """native_decide-backed proofs should be rejected even when the axioms are whitelisted."""
    config = dict(default_config)
    config["allowedAxioms"] = [
        *config["allowedAxioms"],
        "Lean.ofReduceBool",
        "Lean.ofReduceNat",
        "Lean.trustCompiler",
    ]
    # Disable replay so the checker mirrors the current production configuration
    # where native_decide slips through; we want this test to fail until we add
    # an explicit denylist.
    config["enableReplay"] = False

    result = verifier.verify_theorem(
        "LeanTestProject.NativeDecide.Leak", "exploit_theorem", config=config
    )

    assert not result.success
    assert "NoNativeComputation" in result.failed_tests
